<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>

<h1>Evaluation Report — CineCrave</h1>

<p>
This document describes the evaluation methodology used for the CineCrave movie recommendation system.
</p>

<p>
CineCrave is a <strong>content-based, unsupervised recommender system</strong>. As a result,
traditional accuracy-based metrics are intentionally not used. Evaluation instead focuses on
<strong>system behavior, qualitative relevance, ranking impact, and failure case analysis</strong>,
which better reflects real-world recommender system assessment when labeled data is unavailable.
</p>

<hr>

<h2>1. Evaluation Scope</h2>

<p>The system does not rely on:</p>

<ul>
  <li>Explicit user ratings</li>
  <li>Click-through or watch-time logs</li>
  <li>Ground-truth relevance labels</li>
</ul>

<p>
Because of this, metrics such as <strong>accuracy, precision, recall, RMSE, MAP, or NDCG</strong>
would be misleading and are deliberately excluded.
</p>

<p>The evaluation is designed to answer the following questions:</p>

<ul>
  <li>Are the recommendations semantically relevant?</li>
  <li>Does personalization meaningfully change recommendations?</li>
  <li>Does ranking improve result quality beyond raw similarity?</li>
  <li>Where does the system fail, and why?</li>
</ul>

<hr>

<h2>2. Qualitative Relevance Evaluation</h2>

<h3>Methodology</h3>

<p>
For a given query movie, the top-N recommendations are manually inspected to assess semantic
relevance based on narrative themes, setting, genre overlap, and high-level story structure.
</p>

<p>
This human-in-the-loop approach is appropriate for early-stage content-based systems where
labeled relevance data is unavailable.
</p>

<h3>Example: Titanic</h3>

<p><strong>Observed Recommendation Themes:</strong></p>

<ul>
  <li>Maritime settings</li>
  <li>Survival-driven narratives</li>
  <li>Disaster-centered emotional arcs</li>
</ul>

<pre>
Poseidon
All Is Lost
The Reef
Fool's Gold
</pre>

<p>
Although <em>Titanic</em> is often categorized as a romance film, its metadata emphasizes disaster
and survival elements. The observed recommendations are therefore considered
<strong>semantically consistent</strong> with the system’s content representation.
</p>

<hr>

<h2>3. Ranking Impact Analysis</h2>

<h3>Objective</h3>

<p>
To evaluate whether the hybrid ranking layer improves recommendation quality beyond raw
semantic similarity.
</p>

<h3>Comparison Setup</h3>

<ul>
  <li><strong>Baseline:</strong> FAISS-based nearest-neighbor retrieval using embedding similarity</li>
  <li><strong>Enhanced:</strong> Hybrid ranking combining semantic similarity, popularity bias, and novelty</li>
</ul>

<h3>Observations</h3>

<ul>
  <li>Raw FAISS results often exhibited high similarity but limited diversity</li>
  <li>Hybrid ranking reduced repetition among recommendations</li>
  <li>Ranked results showed improved thematic separation</li>
</ul>

<p>
This confirms that <strong>ranking logic plays a critical role</strong> in overall recommendation quality.
</p>

<hr>

<h2>4. Personalization Validation</h2>

<h3>Methodology</h3>

<p>
User-based recommendations were generated by computing a <strong>user embedding</strong> as the average
of embeddings from previously watched movies. These recommendations were compared against
item-based results.
</p>

<h3>Observations</h3>

<ul>
  <li>Personalized recommendations shifted toward themes consistent with user history</li>
  <li>Output behavior differed meaningfully from non-personalized recommendations</li>
</ul>

<p>
This validates that <strong>user embedding aggregation influences retrieval behavior</strong> as intended.
</p>

<hr>

<h2>5. Failure Case Analysis</h2>

<h3>5.1 Cold-Start Users</h3>

<p>
Users without history default to item-based recommendations, limiting personalization.
</p>

<h3>5.2 Sparse Metadata</h3>

<p>
Movies with limited textual descriptions or metadata produce weaker embeddings, reducing
recommendation quality.
</p>

<h3>5.3 Popularity Bias</h3>

<p>
Highly popular movies can dominate ranked results if ranking weights are not carefully tuned.
</p>

<p>
These limitations are inherent to content-based systems and are documented transparently.
</p>

<hr>

<h2>6. Key Findings</h2>

<ul>
  <li>Recommendation quality is best assessed through behavioral and qualitative analysis</li>
  <li>Hybrid ranking significantly improves output quality over raw similarity retrieval</li>
  <li>Debugging and correctness had greater impact than model changes</li>
  <li>Observed behavior aligns with system design assumptions</li>
</ul>

<hr>

<h2>7. Future Evaluation Improvements</h2>

<ul>
  <li>Learning-to-rank models using interaction data</li>
  <li>Implicit feedback evaluation</li>
  <li>Offline ranking metrics</li>
  <li>Online A/B testing</li>
</ul>

<hr>

<h2>Conclusion</h2>

<p>
This evaluation prioritizes <strong>understanding system behavior over reporting arbitrary metrics</strong>.
The methodology reflects real-world recommender system evaluation practices where interpretability,
correctness, and known limitations are critical.
</p>

</body>
</html>
